//qwen 조사

qwen 2.5 

fine-tuning 준비

1. 데이터셋 준비
1-1. 데이터 셋 토크나이징 과정 시행
2. 사전 학습(전처리)
3. 본학습
4. 정확도 체크
5. 테스트
6. 완료


데이터 셋 준비

1. 데이터 수집

2. 데이터 전처리

3. 토크나이징

// 데이터셋 생성
import torch
from transformers import BertTokenizer, DataCollatorWithPadding
from datasets import load_dataset

# 1. 데이터 수집 (예시로 Hugging Face의 `glue` 라이브러리에서 IMDB 영화 리뷰 데이터셋 사용)
dataset = load_dataset('imdb')

# 2. 데이터 전처리
def preprocess_function(examples):
    # 토크나이저를 통해 단어들을 토큰화합니다.
    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)

tokenized_datasets = dataset.map(preprocess_function, batched=True)

# 3. 토큰화된 데이터셋을 PyTorch DataLoader로 변환
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
train_loader = torch.utils.data.DataLoader(
    tokenized_datasets['train'], 
    collate_fn=data_collator, 
    batch_size=16
)

# BERT 토크나이저를 초기화합니다.
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# 4. 모델과 트레이너 설정
from transformers import Trainer, TrainingArguments

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01
)

trainer = Trainer(
    model=model, 
    args=training_args, 
    train_dataset=train_loader.dataset,
    eval_dataset=None  # 테스트셋이 필요하면 추가할 수 있습니다.
)

# 5. 학습 시작
trainer.train()

# 6. 모델 저장
model.save_pretrained('./fine-tuned-bert')
tokenizer.save_pretrained('./fine-tuned-bert')

print("Fine-tuning 완료!")



// 준비
pip install transformers datasets torch

// 학습 코드
import pandas as pd
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

# 데이터 준비 (예시용)
data = {
    'text': ["I love this movie", "This is a terrible experience", "I will not come back again"],
    'label': [1, 0, 0]  # 1은 양성 클래스, 0은 음성 클래스
}
df = pd.DataFrame(data)

# 훈련 데이터와 검증 데이터로 나누기
from sklearn.model_selection import train_test_split
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# 토크나이저 설정
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_function(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)

# 데이터 전처리
train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding='max_length', max_length=128)
val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding='max_length', max_length=128)

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    
    def __len__(self):
        return len(self.labels)

train_dataset = TextDataset(train_encodings, train_df['label'].tolist())
val_dataset = TextDataset(val_encodings, val_df['label'].tolist())

# 모델 설정
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# 학습 매개변수 설정
training_args = TrainingArguments(
    output_dir='./results',          # 실험 결과를 저장할 디렉토리
    evaluation_strategy="epoch",     # 각 epoch마다 검증 수행
    learning_rate=2e-5,              # 학습률
    per_device_train_batch_size=16,  # 배치 크기
    per_device_eval_batch_size=16,
    num_train_epochs=3,              # 훈련 epochs 수
    weight_decay=0.01                # L2 정규화 매개변수
)

# Trainer 설정 및 학습 시작
trainer = Trainer(
    model=model,                         # 모델
    args=training_args,                  # 학습 매개변수
    train_dataset=train_dataset,         # 훈련 데이터셋
    eval_dataset=val_dataset             # 검증 데이터셋
)

# 학습 수행
trainer.train()

# 모델 저장
model.save_pretrained('./fine-tuned-bert')
tokenizer.save_pretrained('./fine-tuned-bert')

print("Fine-tuning 완료!")
